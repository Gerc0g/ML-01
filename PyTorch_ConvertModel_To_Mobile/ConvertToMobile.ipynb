{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаги для подготовки модели к развертыванию:\n",
    "Оптимизация модели:\n",
    "\n",
    "Уменьшение размера модели: Это может включать в себя уменьшение количества слоев или нейронов в каждом слое. Для более сложных моделей можно применять техники квантования или прунинга.\n",
    "Квантование: Позволяет снизить точность весов модели, например, с 32-битных чисел с плавающей точкой до 8-битных целых чисел, с минимальной потерей качества.\n",
    "Проверка производительности модели: Убедитесь, что оптимизация не привела к значительной потере точности.\n",
    "Использование подходящих инструментов:\n",
    "\n",
    "Для iOS (устройства Apple) часто используется Core ML. Вы можете конвертировать обученную модель в формат Core ML с помощью инструмента, такого как coremltools.\n",
    "Для Android можно использовать TensorFlow Lite. TensorFlow предоставляет инструменты для конвертации моделей в формат .tflite, который эффективно работает на мобильных устройствах.\n",
    "Тестирование на устройствах:\n",
    "\n",
    "После конвертации и интеграции модели в приложение проведите тестирование на разных устройствах, чтобы убедиться в её работоспособности и производительности.\n",
    "Особое внимание уделите скорости инференса и потреблению памяти, так как эти факторы критически важны для мобильных устройств."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестирования производительности и ресурсоэффективности модели машинного обучения на мобильных устройствах, важно оценить несколько ключевых параметров. Эти параметры включают время выполнения (инференса), потребление памяти, использование процессора и, при необходимости, использование графического процессора (GPU). Вот несколько подходов и инструментов, которые можно использовать для тестирования:\n",
    "\n",
    "1. Время выполнения (скорость инференса)\n",
    "Использование профайлеров:\n",
    "Android Studio Profiler: Позволяет отслеживать использование CPU, памяти и сетевую активность вашего приложения в реальном времени. Вы можете использовать его для измерения времени, необходимого для выполнения инференса вашей модели.\n",
    "Xcode Instruments: Инструменты для профилирования приложений на iOS, которые предоставляют детальную информацию о производительности приложения, включая время выполнения задач.\n",
    "2. Потребление памяти\n",
    "Memory Profiler в Android Studio и Instruments в Xcode: Эти инструменты позволяют отслеживать использование памяти вашим приложением. Они могут помочь выявить утечки памяти и другие проблемы, которые могут возникнуть при выполнении инференса модели.\n",
    "3. Энергопотребление\n",
    "Battery Profiler в Android Studio: Предоставляет информацию о том, как ваше приложение влияет на заряд батареи устройства.\n",
    "Energy Log в Instruments для iOS: Помогает определить, как использование вашего приложения влияет на энергопотребление устройства.\n",
    "4. Тестирование на реальных устройствах\n",
    "Важно провести тестирование на реальных устройствах с различными характеристиками (в том числе на устройствах с низкими характеристиками), чтобы убедиться в том, что приложение работает корректно и эффективно использует ресурсы.\n",
    "Практические шаги для тестирования:\n",
    "Интеграция модели в приложение: После конвертации модели в соответствующий формат (например, .tflite или .mlmodel) интегрируйте её в ваше мобильное приложение.\n",
    "\n",
    "Разработка тестовых сценариев: Создайте сценарии использования, которые имитируют реальные условия работы приложения. Это может включать инференс на живом видео, анализ изображений, обработку текста и т.д.\n",
    "\n",
    "Запуск тестов и сбор данных: Используйте вышеупомянутые профайлеры и инструменты для сбора данных о производительности и ресурсоэффективности вашей модели в различных условиях.\n",
    "\n",
    "Анализ результатов и оптимизация: Анализируйте полученные данные, идентифицируйте узкие места и проблемы. В зависимости от результатов, вы можете потребоваться дополнительная оптимизация модели или изменение логики приложения.\n",
    "\n",
    "Тестирование на разных устройствах и в различных условиях поможет убедиться, что ваше приложение эффективно работает и оптимально использует ресурсы мобильного устройства, обеспечивая при этом необходимое качество инференса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конвертация модели PyTorch в формат, совместимый с iOS, обычно включает в себя несколько шагов, включая оптимизацию модели и её экспорт в формат Core ML, который поддерживается устройствами Apple. Вот основные этапы процесса:\n",
    "\n",
    "    Оптимизация модели PyTorch: Перед тем как конвертировать модель, важно убедиться, что она оптимизирована для эффективности на мобильных устройствах. Это может включать в себя уменьшение размера модели, прунинг (удаление неиспользуемых весов) и квантизацию.\n",
    "\n",
    "    Экспорт в ONNX: Модель PyTorch можно экспортировать в формат ONNX, который является промежуточным стандартом для представления моделей машинного обучения. Это делается с помощью функции torch.onnx.export.\n",
    "\n",
    "    Конвертация ONNX в Core ML: Для конвертации из ONNX в Core ML можно использовать инструмент onnx-coreml. Этот инструмент поддерживает прямую конвертацию и позволяет дополнительно адаптировать модель под нужды iOS.\n",
    "\n",
    "    Интеграция в iOS приложение: После конвертации модели в Core ML, следующим шагом будет её интеграция в iOS-приложение. Это включает добавление .mlmodel файла в проект Xcode и использование Core ML API для работы с моделью.\n",
    "\n",
    "    Тестирование на устройстве: Важно тестировать модель на реальном устройстве, чтобы убедиться, что она работает корректно и эффективно. Это включает проверку производительности и точности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "#import onnx\n",
    "#from onnx_coreml import convert\n",
    "#from torchsummary import summary\n",
    "import coremltools as ct\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "#from linRegres import MLPModel,LinearRegressionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LinearRegressionModel                    --                        --\n",
       "├─Linear: 1-1                            --                        2\n",
       "==========================================================================================\n",
       "Total params: 2\n",
       "Trainable params: 2\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.jit.load('model_traced.pt')\n",
    "summary(loaded_model, input_size=(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Support for converting Torch Script Models is experimental. If possible you should use a traced model for conversion.\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/1 [00:00<?, ? ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<?, ? passes/s]\n",
      "Running MIL default pipeline:   0%|          | 0/69 [00:00<?, ? passes/s]c:\\Users\\Логутов Егор\\Desktop\\Github\\ML-01\\PyTorch_ConvertModel_To_Mobile\\venvConvert\\Lib\\site-packages\\coremltools\\converters\\mil\\mil\\passes\\defs\\preprocess.py:239: UserWarning: Input, 'x.1', of the source model, has been renamed to 'x_1' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████| 69/69 [00:00<00:00, 5738.90 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<?, ? passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 3/3 [00:00<?, ? ops/s]\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(1, 1)\n",
    "# Конвертируем TorchScript модель в Core ML\n",
    "mlmodel = ct.convert(\n",
    "    loaded_model,\n",
    "    inputs=[ct.TensorType(shape=example_input.shape)],  # Указываем тип и форму входных данных\n",
    "    convert_to='neuralnetwork'\n",
    ")\n",
    "\n",
    "\n",
    "# Сохраняем модель в файл\n",
    "mlmodel.save(\"MyModel.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "specificationVersion: 4\n",
       "description {\n",
       "  input {\n",
       "    name: \"x_1\"\n",
       "    type {\n",
       "      multiArrayType {\n",
       "        shape: 1\n",
       "        shape: 1\n",
       "        dataType: FLOAT32\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  output {\n",
       "    name: \"linear_0\"\n",
       "    type {\n",
       "      multiArrayType {\n",
       "        dataType: FLOAT32\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  metadata {\n",
       "    userDefined {\n",
       "      key: \"com.github.apple.coremltools.source\"\n",
       "      value: \"torch==2.2.2+cpu\"\n",
       "    }\n",
       "    userDefined {\n",
       "      key: \"com.github.apple.coremltools.source_dialect\"\n",
       "      value: \"TorchScript\"\n",
       "    }\n",
       "    userDefined {\n",
       "      key: \"com.github.apple.coremltools.version\"\n",
       "      value: \"7.1\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "neuralNetwork {\n",
       "  layers {\n",
       "    name: \"linear_0\"\n",
       "    input: \"x_1\"\n",
       "    output: \"linear_0\"\n",
       "    innerProduct {\n",
       "      inputChannels: 1\n",
       "      outputChannels: 1\n",
       "      hasBias: true\n",
       "      weights {\n",
       "        floatValue: 2.0053625\n",
       "      }\n",
       "      bias {\n",
       "        floatValue: 0.9431033\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  arrayInputShapeMapping: EXACT_ARRAY_MAPPING\n",
       "  imageInputShapeMapping: RANK4_IMAGE_MAPPING\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузите модель Core ML\n",
    "mlmodel = ct.models.MLModel('MyModel.mlmodel')\n",
    "\n",
    "# Вывод сводки модели\n",
    "mlmodel.get_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LinearRegressionModel                    --\n",
       "├─Linear: 1-1                            2\n",
       "=================================================================\n",
       "Total params: 2\n",
       "Trainable params: 2\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание экземпляра модели\n",
    "model = LinearRegressionModel(input_size=1, output_size=1)\n",
    "\n",
    "# Загрузка весов\n",
    "path_to_weights = 'model_params.pth'\n",
    "model_weights = torch.load(path_to_weights)\n",
    "\n",
    "# Применение загруженных весов к модели\n",
    "model.load_state_dict(model_weights)\n",
    "\n",
    "# Перевод модели в режим оценки (если модель используется для инференса)\n",
    "model.eval()\n",
    "\n",
    "# Вывод сводной информации о модели\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"milinternal\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you're converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)  \u001b[38;5;66;03m# Измените размер в соответствии с вашей моделью\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Конвертация модели\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m coreml_model \u001b[38;5;241m=\u001b[39m \u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageType\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_layout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Настройте параметры масштабирования и смещения\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m coreml_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyModel.mlmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Логутов Егор\\Desktop\\Github\\ML-01\\PyTorch convert to IOS and Android\\MyVenv\\Lib\\site-packages\\coremltools\\converters\\_converters_entry.py:527\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, source, inputs, outputs, classifier_config, minimum_deployment_target, convert_to, compute_precision, skip_model_load, compute_units, package_dir, debug, pass_pipeline)\u001b[0m\n\u001b[0;32m    525\u001b[0m _check_deployment_target(minimum_deployment_target)\n\u001b[0;32m    526\u001b[0m outputs_as_strings, outputs_as_tensor_or_image_types \u001b[38;5;241m=\u001b[39m _validate_outputs_argument(outputs)\n\u001b[1;32m--> 527\u001b[0m exact_source \u001b[38;5;241m=\u001b[39m \u001b[43m_determine_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutputs_as_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutputs_as_tensor_or_image_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m source_dialect \u001b[38;5;241m=\u001b[39m _determine_source_dialect(model, exact_source)\n\u001b[0;32m    532\u001b[0m exact_target \u001b[38;5;241m=\u001b[39m _determine_target(convert_to, minimum_deployment_target)\n",
      "File \u001b[1;32mc:\\Users\\Логутов Егор\\Desktop\\Github\\ML-01\\PyTorch convert to IOS and Android\\MyVenv\\Lib\\site-packages\\coremltools\\converters\\_converters_entry.py:977\u001b[0m, in \u001b[0;36m_determine_source\u001b[1;34m(model, source, output_names, outputs_as_tensor_or_image_types, output_argument_as_specified_by_user)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilinternal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    968\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to determine the type of the model, i.e. the source framework. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease provide the value of argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, from one of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have `tensorflow==1.14` installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    976\u001b[0m )\n\u001b[1;32m--> 977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to determine the type of the model, i.e. the source framework. Please provide the value of argument \"source\", from one of [\"tensorflow\", \"pytorch\", \"milinternal\"]. Note that model conversion requires the source package that generates the model. Please make sure you have the appropriate version of source package installed. E.g., if you're converting model originally trained with TensorFlow 1.14, make sure you have `tensorflow==1.14` installed."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Подготовка примера входных данных\n",
    "example_input = torch.rand(1, 3, 224, 224)  # Измените размер в соответствии с вашей моделью\n",
    "\n",
    "# Конвертация модели\n",
    "coreml_model = ct.convert(\n",
    "    model,\n",
    "    inputs=[ct.ImageType(color_layout=\"RGB\", scale=1/255.0, bias=[0, 0, 0])]  # Настройте параметры масштабирования и смещения\n",
    ")\n",
    "\n",
    "\n",
    "coreml_model.save('MyModel.mlmodel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
